{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FTeLwWc8MMBO"
   },
   "source": [
    "# Text Prediction and Generation using LSTM Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "ZNalDTJmMjX-",
    "outputId": "bc0ca233-d918-444a-df8a-a85ebea5738d"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Code to read google drive files into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________\n",
    "## Google drive code only\n",
    "Only run the following sections in a colaboratory shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google drive code only\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "ujVuCb3iRW8G",
    "outputId": "8cdcc0fb-be62-4737-9c03-9a805fc45e22"
   },
   "outputs": [],
   "source": [
    "# Google drive code only\n",
    "%%time\n",
    "def importFile(fileName: str, fileID: str):\n",
    "  '''Imports a file into the Colaboratory workspace. The fileID can be \n",
    "     found in the file's Share Link'''\n",
    "  print(\"Grabbing file \" + str(fileName) + \" with id = \" + str(fileID)) # Verify that you have everything after '='\n",
    "  downloaded = drive.CreateFile({'id':fileID}) \n",
    "  downloaded.GetContentFile(fileName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "colab_type": "code",
    "id": "r7xpitMqO8bN",
    "outputId": "2526d459-b32c-4df9-995e-c6e03412bebc"
   },
   "outputs": [],
   "source": [
    "# Google drive code only\n",
    "# Import all the datasets we are going to use for our project\n",
    "importFile('TheLordOfTheRings_Book1.txt', '1crAeSigOaQcT62W7EjcwrKcIVBx5ayeh')\n",
    "importFile('GoogleNews-vectors-negative300.bin', '1zzUeVFsRYw3lWe6kjk1nccywCkMDp-AY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Google drive code\n",
    "_________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "colab_type": "code",
    "id": "3ODs77INUaJl",
    "outputId": "f470718b-c04a-4ac2-8a12-edcf2fbe4615"
   },
   "outputs": [],
   "source": [
    "# Load top 1 million (out of 3 million) word embeddings from the binary file.\n",
    "# These vectors are 300 dimensions large and are created using the word2vec algorithm.\n",
    "# The model was trained on the GoogleNews corpus, which is a similar size to the English version of Wikipedia.\n",
    "# We can only load 1 million vectors because we run out of RAM.\n",
    "\n",
    "# %%time\n",
    "import gensim\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', limit=1000000, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_WqVQBraIN7"
   },
   "source": [
    "### Word Embedding Examples\n",
    "\n",
    "When we import the binary file using the gensim library, we can easily explore interesting properties of word embeddings. Much like a latent vector produced using an autoencoder, each embedding holds meaningful information about each word and the context in which it is used. The following examples explore the latent space in which these vectors reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "pC_Da54bPmGt",
    "outputId": "9a8f8143-406b-41b2-942f-22d48e41bcbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('teenage_girl', 0.7674504518508911), ('teenager', 0.7674364447593689), ('toddler', 0.701943576335907)]\n",
      "[('striped_bass', 0.47242963314056396), ('bluefin', 0.4493907392024994), ('tunas', 0.4461327791213989)]\n",
      "[('woman', 0.9972377419471741)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We can perform interesting vector addition/subtraction\n",
    "print(embeddings.most_similar(positive=[\"boy\", \"girl\"], topn=3))\n",
    "print(embeddings.most_similar(positive=[\"fish\"], negative=[\"water\"], topn=3)) # What is a fish without water?\n",
    "\n",
    "# We can extract nearby points. This feature will be useful because our model \n",
    "# won't output vectors that exactly correspond to existing vectors.\n",
    "v = np.copy(embeddings[\"woman\"])\n",
    "v[0] += 0.2\n",
    "print(embeddings.most_similar(positive=[v], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bF-OhBj7aFDY"
   },
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1e9PxMNKjTXDx7KZDxHpC4ENmz0sXHl_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuvUj83Fb1ug"
   },
   "source": [
    "## Preprocessing \n",
    "This is were we generate the training and validation data for our LSTM. The following functions will be used in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mz9lUGnqWNGd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from nltk import ngrams\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "def tokenizeFile(fileName: str) -> list:\n",
    "  '''Will take in the name of a txt file located in the base directory \n",
    "     of the drive and return a list of terms'''\n",
    "  words = []\n",
    "  with open(fileName, 'r', encoding=\"ISO-8859-1\") as myTxtFile:\n",
    "    for line in myTxtFile:\n",
    "      lineWords = [word for word in re.split(r'[^a-zA-Z]', line.strip()) if word]\n",
    "      words += lineWords\n",
    "  return words\n",
    "\n",
    "\n",
    "def flipFirstChar(word: str) -> str:\n",
    "  '''Capitalizes/Un-capitalizes the first character of the word'''\n",
    "  if word[0].isupper():\n",
    "    return word[0].lower() + word[1:]\n",
    "  else:\n",
    "    return word[0].upper() + word[1:]\n",
    "\n",
    "\n",
    "def wordsToVectors(words: list) -> (list, list):\n",
    "  '''Convert a list of terms into a list of word embeddings. \n",
    "     We also keep track of which terms cannot be paired with an embedding.'''\n",
    "  vectors = []\n",
    "  unknownWords = set()\n",
    "  for word in words:\n",
    "    if word in embeddings:\n",
    "        vectors.append(embeddings[word])\n",
    "    elif flipFirstChar(word) in embeddings:\n",
    "        vectors.append(embeddings[flipFirstChar(word)])\n",
    "    else:\n",
    "        unknownWords.add(word)\n",
    "  return vectors, unknownWords\n",
    "\n",
    "\n",
    "def phrasesToVectors(phrases: list, summary=True) -> (list, list):\n",
    "    '''Calls wordsToVectors() on a list of lists. This function will be called\n",
    "       after data is split into ngrams. Each phrase in the code below operations\n",
    "       on a single ngram.'''\n",
    "    phrase_vectors = []\n",
    "    all_unknown_words = set()\n",
    "    a = 1\n",
    "    for phrase in phrases:\n",
    "        vectors, unknown_words = wordsToVectors(phrase)\n",
    "        phrase_vectors.append(vectors)\n",
    "#         print(np.shape(phrase_vectors))\n",
    "#         if a == 2000:\n",
    "#           break\n",
    "#         else:\n",
    "#           a += 1\n",
    "        all_unknown_words = all_unknown_words.union(unknown_words)  \n",
    "    if summary:\n",
    "      print(\"Number of training phrases: \" + str(len(phrase_vectors)))\n",
    "      print(\"Number of unknown terms: \" + str(len(all_unknown_words)))\n",
    "    print(np.shape(phrase_vectors))\n",
    "    return np.array(phrase_vectors, dtype=np.ndarray), all_unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps:\n",
    "\n",
    "1. Split training data into individual tokens, ie: \"Here is a sentence\" --> \\[\"Here\" \"is\" \"a\" \"sentence\"\\]\n",
    "2. Use nltk to split training data into ngram phrases to feed into our LSTM network\n",
    "    - The model's architecture will automatically adjust based on ngram size\n",
    "3. Shuffle the data\n",
    "4. Split the data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training phrases: 190112\n",
      "Number of unknown terms: 548\n",
      "(190112,)\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = tokenizeFile(\"TheLordOfTheRings_Book1.txt\")\n",
    "# Change this parameter to decrease or increase the size of the training samples\n",
    "# The last element in each phrase is the vector that the LSTM network will\n",
    "# try to predict.\n",
    "N = 5\n",
    "ngrams_words = ngrams(words, N + 1)\n",
    "ngram_vectors, _ = phrasesToVectors(ngrams_words)\n",
    "np.random.shuffle(ngram_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1.00 percent of samples\n"
     ]
    }
   ],
   "source": [
    "# This block of code cuts the data to make training go faster during experimentation\n",
    "total_size = np.size(ngram_vectors)\n",
    "ngram_vectors = ngram_vectors[:total_size//100]\n",
    "adjusted_size = np.size(ngram_vectors)\n",
    "percentage = (adjusted_size/total_size) * 100\n",
    "print(\"Using %.2f percent of samples\" % percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6l2HRpsmaK84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1901,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-9e4edcf062b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# for row in ngram_vectors:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     print(np.shape(row))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram_vectors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_vectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eric\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all input arrays must have the same shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "print(ngram_vectors.shape)\n",
    "# for row in ngram_vectors:\n",
    "#     print(np.shape(row))\n",
    "n = np.stack([ngram_vectors[i] for i in range(np.size(ngram_vectors))])\n",
    "print(np.shape(n))\n",
    "\n",
    "\n",
    "# Split data into training and validation\n",
    "# split = math.floor(np.size(ngram_vectors) * 0.8)\n",
    "# training = ngram_vectors[:split]\n",
    "# validation = ngram_vectors[split:]\n",
    "\n",
    "# Split the sample data phrases into input and expected. For example:\n",
    "# phrase: [This tale grew in the telling] --> [This tale grew in the] [telling]\n",
    "#                                              ^ input data            ^ expected output\n",
    "\n",
    "# print(training.shape)\n",
    "# print(training[0].shape)\n",
    "# print(training[0][:-1, :].shape)\n",
    "# print(training_in.shape)\n",
    "\n",
    "# train_first = np.expand_dims(training[:, 0], axis=1)\n",
    "# train_second = np.expand_dims(training[:, 1], axis=1)\n",
    "# val_first = np.expand_dims(validation[:, 0], axis=1)\n",
    "# val_second = np.expand_dims(validation[:, 1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Explaination\n",
    "- The input shape is 300 because we take in each dimension of the word embedding as input\n",
    "- Sigmoid activations are used because word embeddings are normalized betweeen zero and one\n",
    "- Loss is cosine similarity because word2vec also uses cosine similarity to measure the distance between word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "CymWYziXllRA",
    "outputId": "f20dcb33-4e2d-4a88-cd37-1c7a39b44efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 5, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 5, 300)            721200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5, 300)            90300     \n",
      "=================================================================\n",
      "Total params: 1,532,700\n",
      "Trainable params: 1,532,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()  \n",
    "model.add(LSTM(300, input_shape=(N, 300), return_sequences=True, activation='sigmoid'))\n",
    "model.add(LSTM(300, input_shape=(N, 300), return_sequences=True, activation='sigmoid'))\n",
    "model.add(Dense(300))\n",
    "model.compile(loss='cosine_proximity', optimizer='adam',metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "vgbQwD42gEjL",
    "outputId": "9f8a3045-a541-4ad5-b089-4ab0950fb148"
   },
   "outputs": [],
   "source": [
    "print(np.expand_dims(training[:, 0], axis=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "UaStJMjKWf7Y",
    "outputId": "544df0a1-1134-4b24-da7c-a2996838b73c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(train_first, train_second, epochs=50, batch_size=32, verbose=2, validation_data=(val_first, val_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "f_tzFZDEhjxD",
    "outputId": "72d2f6e8-90b0-4377-bc66-7e7df4c80f44"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "  plt.plot(history.history['acc'],label='train')\n",
    "  if 'val_acc' in history.history:\n",
    "    plt.plot(history.history['val_acc'],label='val')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.legend()\n",
    "  plt.title('Accuracy during Training')\n",
    "  plt.show()\n",
    "  \n",
    "  plt.plot(history.history['loss'],label='train')\n",
    "  if 'val_loss' in history.history:\n",
    "    plt.plot(history.history['val_loss'],label='val')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.title('Loss during Training')\n",
    "  plt.show()\n",
    "\n",
    "#results = model.evaluate(x_test, y_test, verbose=False)\n",
    "#print('Test loss:', results[0])\n",
    "#print('Test accuracy:', results[1])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "7V2LweZcjqfG",
    "outputId": "263de1ad-0b2a-4938-d724-4f2ca761eb67"
   },
   "outputs": [],
   "source": [
    "# test_vec = np.expand_dims(np.array(embeddings[\"A\"]), axis=1)\n",
    "# list_vec = []\n",
    "# list_vec.append(embeddings['A'])\n",
    "# test_vec = np.array(list_vec)\n",
    "# print(test_vec.shape)\n",
    "\n",
    "# vec = np.array(np.array(embeddings[\"A\"]))\n",
    "\n",
    "def getTestingArray(sentence: str):\n",
    "  arr = []\n",
    "  for i in sentence.split():\n",
    "    item = embeddings[i]\n",
    "    arr.append(item)\n",
    "  return np.array(arr)\n",
    "\n",
    "\n",
    "def printPredictions(predictions):\n",
    "  print(predictions.shape)\n",
    "  for i in range(len(predictions)):\n",
    "    print(embeddings.most_similar(positive=[predictions[i][0]], topn=3))\n",
    "\n",
    "    \n",
    "test_input = getTestingArray(\"This tale grew in the telling\")\n",
    "test_input = np.expand_dims(test_input, axis=1)\n",
    "predictions = model.predict(test_input)\n",
    "printPredictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QbRBtgaamGef"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Project Code",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
