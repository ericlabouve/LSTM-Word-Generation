{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from nltk import ngrams\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFile(fileName: str) -> list:\n",
    "    '''Will take in the name of a txt file located in the base directory \n",
    "    of the drive and return a list of tokens. Tokens are defined to be\n",
    "    any nonzero sequence of characters.'''\n",
    "    tokens = []\n",
    "    with open(fileName, 'r', encoding=\"ISO-8859-1\") as file:\n",
    "        tokens = word_tokenize(file.read())\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 4435\n",
      "Unique Tokens: 1261\n",
      "Total ngrams: 4415\n",
      "Total Samples: 4415\n"
     ]
    }
   ],
   "source": [
    "# Cut number of words so we can fit everything in memory\n",
    "tokens_all = tokenizeFile(\"TheLordOfTheRings_Book1.txt\")\n",
    "tokens = tokens_all[:len(tokens_all)//50]\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# Change this parameter to decrease or increase the size of the training samples\n",
    "N = 21\n",
    "ngrams_tokens = list(ngrams(tokens, N))\n",
    "np.random.shuffle(ngrams_tokens)\n",
    "print('Total ngrams: %d' % len(ngrams_tokens))\n",
    "\n",
    "# Combine tokens into training sample phrases\n",
    "ngrams_samples = [\" \".join(x) for x in ngrams_tokens]\n",
    "print(\"Total Samples: %d\" % len(ngrams_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a unique ID to each token and make an inverted index so we can look up terms easily later.\n",
    "\n",
    "Determine the vocabulary size for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences Shape: (4415, 21)\n",
      "Vocabulary size: 1190\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(ngrams_samples)\n",
    "sequencies = np.array(tokenizer.texts_to_sequences(ngrams_samples))\n",
    "# sequencies = tokenizer.texts_to_sequences(ngrams_samples)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Sequences Shape: \" + str(sequencies.shape))\n",
    "print(\"Vocabulary size: %d\" % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, validation, and testing\n",
    "\n",
    "Split into input and expected output where the input is all but the last token and the output is the last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array as ar\n",
    "\n",
    "tr = int(len(sequencies) * 0.9) # Training samples is 90% of data\n",
    "va = int(len(sequencies) * 0.95) # Validation samples is 5% of data \n",
    "te = len(sequencies) # Testing samples is last 5% of data\n",
    "training, validation, testing = sequencies[:tr], sequencies[tr:va], sequencies[va:te]\n",
    "\n",
    "tr_input, tr_output = ar([x[:-1] for x in training]), [x[-1] for x in training]\n",
    "tr_output = to_categorical(tr_output, num_classes=vocab_size)\n",
    "\n",
    "va_input, va_output = [x[:-1] for x in validation], [x[-1] for x in validation]\n",
    "va_output = to_categorical(va_output, num_classes=vocab_size)\n",
    "\n",
    "te_input, te_output = [x[:-1] for x in testing], [x[-1] for x in testing]\n",
    "te_output = to_categorical(te_output, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3973, 20)\n",
      "(3973, 1190)\n"
     ]
    }
   ],
   "source": [
    "print(tr_input.shape)\n",
    "# print(tr_input)\n",
    "print(tr_output.shape)\n",
    "\n",
    "# l = -1\n",
    "# c = 0\n",
    "# for x in sequencies:\n",
    "#     if l == -1:\n",
    "#         l = len(x)\n",
    "#     else:\n",
    "#         if len(x) != l:\n",
    "#             print(c, end=\": \")\n",
    "#             print(len(x), end=\" --- \")\n",
    "#             print(x)\n",
    "#     c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "seq_length = tr_input.shape[1]\n",
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 50)            59500     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 20, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1190)              120190    \n",
      "=================================================================\n",
      "Total params: 330,590\n",
      "Trainable params: 330,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(tr_input, tr_output, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
