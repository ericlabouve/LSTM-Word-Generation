{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from nltk import ngrams\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeFile(fileName: str) -> list:\n",
    "    '''Will take in the name of a txt file located in the base directory \n",
    "    of the drive and return a list of tokens. Tokens are defined to be\n",
    "    any nonzero sequence of characters.'''\n",
    "    tokens = []\n",
    "    with open(fileName, 'r', encoding=\"ISO-8859-1\") as file:\n",
    "        tokens = word_tokenize(file.read())\n",
    "    return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens: 4435\n",
      "Unique Tokens: 1261\n",
      "Total ngrams: 4406\n",
      "Total Samples: 4406\n"
     ]
    }
   ],
   "source": [
    "# Cut number of words so we can fit everything in memory\n",
    "tokens_all = tokenizeFile(\"TheLordOfTheRings_Book1.txt\")\n",
    "tokens = tokens_all[:len(tokens_all)//50]\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "\n",
    "# Change this parameter to decrease or increase the size of the training samples\n",
    "N = 30\n",
    "ngrams_tokens = list(ngrams(tokens, N))\n",
    "np.random.shuffle(ngrams_tokens)\n",
    "print('Total ngrams: %d' % len(ngrams_tokens))\n",
    "\n",
    "# Combine tokens into training sample phrases\n",
    "ngrams_samples = [\" \".join(x) for x in ngrams_tokens]\n",
    "print(\"Total Samples: %d\" % len(ngrams_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign a unique ID to each token and make an inverted index so we can look up terms easily later.\n",
    "\n",
    "Determine the vocabulary size for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences Shape: (4406, 30)\n",
      "Vocabulary size: 1190\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(ngrams_samples)\n",
    "sequencies = np.array(tokenizer.texts_to_sequences(ngrams_samples))\n",
    "# sequencies = tokenizer.texts_to_sequences(ngrams_samples)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Sequences Shape: \" + str(sequencies.shape))\n",
    "print(\"Vocabulary size: %d\" % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, validation, and testing\n",
    "\n",
    "Split into input and expected output where the input is all but the last token and the output is the last token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array as ar\n",
    "\n",
    "tr = int(len(sequencies) * 0.9) # Training samples is 90% of data\n",
    "va = int(len(sequencies) * 0.95) # Validation samples is 5% of data \n",
    "te = len(sequencies) # Testing samples is last 5% of data\n",
    "training, validation, testing = sequencies[:tr], sequencies[tr:va], sequencies[va:te]\n",
    "\n",
    "tr_input, tr_output = ar([x[:-1] for x in training]), [x[-1] for x in training]\n",
    "tr_output = to_categorical(tr_output, num_classes=vocab_size)\n",
    "\n",
    "va_input, va_output = ar([x[:-1] for x in validation]), [x[-1] for x in validation]\n",
    "va_output = to_categorical(va_output, num_classes=vocab_size)\n",
    "\n",
    "te_input, te_output = ar([x[:-1] for x in testing]), [x[-1] for x in testing]\n",
    "te_output = to_categorical(te_output, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "seq_length = tr_input.shape[1]\n",
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 29, 300)           357000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 29, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 300)               90300     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1190)              358190    \n",
      "=================================================================\n",
      "Total params: 2,247,890\n",
      "Trainable params: 2,247,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 300, input_length=seq_length))\n",
    "model.add(LSTM(300, return_sequences=True))\n",
    "model.add(LSTM(300))\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3965 samples, validate on 220 samples\n",
      "Epoch 1/100\n",
      "3965/3965 [==============================] - 12s 3ms/step - loss: 6.1290 - acc: 0.0633 - val_loss: 5.8044 - val_acc: 0.0591\n",
      "Epoch 2/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 5.6057 - acc: 0.0678 - val_loss: 5.9320 - val_acc: 0.0591\n",
      "Epoch 3/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 5.4021 - acc: 0.0890 - val_loss: 5.7602 - val_acc: 0.0773\n",
      "Epoch 4/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 5.1864 - acc: 0.1034 - val_loss: 5.8424 - val_acc: 0.0909\n",
      "Epoch 5/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 5.0544 - acc: 0.1100 - val_loss: 5.9152 - val_acc: 0.0909\n",
      "Epoch 6/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.9360 - acc: 0.1160 - val_loss: 6.0030 - val_acc: 0.1091\n",
      "Epoch 7/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.7995 - acc: 0.1294 - val_loss: 6.1126 - val_acc: 0.1091\n",
      "Epoch 8/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.6628 - acc: 0.1359 - val_loss: 6.2341 - val_acc: 0.1273\n",
      "Epoch 9/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.5405 - acc: 0.1521 - val_loss: 6.4008 - val_acc: 0.1273\n",
      "Epoch 10/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.4220 - acc: 0.1654 - val_loss: 6.4885 - val_acc: 0.1091\n",
      "Epoch 11/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.3119 - acc: 0.1745 - val_loss: 6.7097 - val_acc: 0.1136\n",
      "Epoch 12/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.2122 - acc: 0.1831 - val_loss: 6.8008 - val_acc: 0.1136\n",
      "Epoch 13/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.1096 - acc: 0.1914 - val_loss: 7.0300 - val_acc: 0.0955\n",
      "Epoch 14/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 4.0082 - acc: 0.2066 - val_loss: 7.1860 - val_acc: 0.1091\n",
      "Epoch 15/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.9190 - acc: 0.2088 - val_loss: 7.3797 - val_acc: 0.1045\n",
      "Epoch 16/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.8055 - acc: 0.2214 - val_loss: 7.4252 - val_acc: 0.0909\n",
      "Epoch 17/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.6907 - acc: 0.2353 - val_loss: 7.5679 - val_acc: 0.0909\n",
      "Epoch 18/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.5563 - acc: 0.2446 - val_loss: 7.7108 - val_acc: 0.0773\n",
      "Epoch 19/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.4529 - acc: 0.2522 - val_loss: 8.0153 - val_acc: 0.0955\n",
      "Epoch 20/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.3095 - acc: 0.2608 - val_loss: 8.0875 - val_acc: 0.0909\n",
      "Epoch 21/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.1610 - acc: 0.2789 - val_loss: 8.2115 - val_acc: 0.0955\n",
      "Epoch 22/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 3.0199 - acc: 0.2895 - val_loss: 8.5127 - val_acc: 0.0864\n",
      "Epoch 23/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 2.8847 - acc: 0.3097 - val_loss: 8.5783 - val_acc: 0.0864\n",
      "Epoch 24/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 2.7260 - acc: 0.3296 - val_loss: 8.7336 - val_acc: 0.0909\n",
      "Epoch 25/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 2.5856 - acc: 0.3554 - val_loss: 8.8469 - val_acc: 0.0727\n",
      "Epoch 26/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 2.4533 - acc: 0.3796 - val_loss: 8.9552 - val_acc: 0.0955\n",
      "Epoch 27/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 2.2852 - acc: 0.4174 - val_loss: 9.0895 - val_acc: 0.0864\n",
      "Epoch 28/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 2.1388 - acc: 0.4502 - val_loss: 9.2989 - val_acc: 0.0636\n",
      "Epoch 29/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.9971 - acc: 0.4837 - val_loss: 9.3947 - val_acc: 0.0682\n",
      "Epoch 30/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.8482 - acc: 0.5082 - val_loss: 9.4781 - val_acc: 0.0636\n",
      "Epoch 31/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.6865 - acc: 0.5533 - val_loss: 9.7884 - val_acc: 0.0636\n",
      "Epoch 32/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.5614 - acc: 0.5786 - val_loss: 9.7852 - val_acc: 0.0636\n",
      "Epoch 33/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.4174 - acc: 0.6184 - val_loss: 9.8646 - val_acc: 0.0727\n",
      "Epoch 34/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.2990 - acc: 0.6512 - val_loss: 10.0830 - val_acc: 0.0682\n",
      "Epoch 35/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.1585 - acc: 0.6908 - val_loss: 10.1470 - val_acc: 0.0727\n",
      "Epoch 36/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 1.0107 - acc: 0.7422 - val_loss: 10.4092 - val_acc: 0.0545\n",
      "Epoch 37/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.8913 - acc: 0.7718 - val_loss: 10.4889 - val_acc: 0.0727\n",
      "Epoch 38/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.7857 - acc: 0.8043 - val_loss: 10.4986 - val_acc: 0.0727\n",
      "Epoch 39/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.6754 - acc: 0.8469 - val_loss: 10.7180 - val_acc: 0.0636\n",
      "Epoch 40/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.5902 - acc: 0.8683 - val_loss: 10.8673 - val_acc: 0.0682\n",
      "Epoch 41/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.5187 - acc: 0.8842 - val_loss: 11.0278 - val_acc: 0.0636\n",
      "Epoch 42/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.4516 - acc: 0.9039 - val_loss: 11.1707 - val_acc: 0.0591\n",
      "Epoch 43/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.3913 - acc: 0.9251 - val_loss: 11.2311 - val_acc: 0.0636\n",
      "Epoch 44/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.3102 - acc: 0.9458 - val_loss: 11.3664 - val_acc: 0.0591\n",
      "Epoch 45/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.2660 - acc: 0.9586 - val_loss: 11.4495 - val_acc: 0.0591\n",
      "Epoch 46/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.2260 - acc: 0.9652 - val_loss: 11.4240 - val_acc: 0.0682\n",
      "Epoch 47/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.1783 - acc: 0.9803 - val_loss: 11.5833 - val_acc: 0.0682\n",
      "Epoch 48/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.1307 - acc: 0.9884 - val_loss: 11.6249 - val_acc: 0.0591\n",
      "Epoch 49/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0993 - acc: 0.9942 - val_loss: 11.7280 - val_acc: 0.0727\n",
      "Epoch 50/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0718 - acc: 0.9977 - val_loss: 11.7971 - val_acc: 0.0591\n",
      "Epoch 51/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0567 - acc: 0.9987 - val_loss: 11.7889 - val_acc: 0.0727\n",
      "Epoch 52/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0509 - acc: 0.9982 - val_loss: 11.9085 - val_acc: 0.0727\n",
      "Epoch 53/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0369 - acc: 0.9997 - val_loss: 11.9053 - val_acc: 0.0727\n",
      "Epoch 54/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0290 - acc: 1.0000 - val_loss: 11.9615 - val_acc: 0.0727\n",
      "Epoch 55/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0247 - acc: 1.0000 - val_loss: 12.0118 - val_acc: 0.0727\n",
      "Epoch 56/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0213 - acc: 1.0000 - val_loss: 12.0468 - val_acc: 0.0773\n",
      "Epoch 57/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0186 - acc: 1.0000 - val_loss: 12.0740 - val_acc: 0.0727\n",
      "Epoch 58/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0165 - acc: 1.0000 - val_loss: 12.0945 - val_acc: 0.0773\n",
      "Epoch 59/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0147 - acc: 1.0000 - val_loss: 12.1309 - val_acc: 0.0773\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0131 - acc: 1.0000 - val_loss: 12.1827 - val_acc: 0.0773\n",
      "Epoch 61/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0116 - acc: 1.0000 - val_loss: 12.1996 - val_acc: 0.0727\n",
      "Epoch 62/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0105 - acc: 1.0000 - val_loss: 12.2246 - val_acc: 0.0727\n",
      "Epoch 63/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0094 - acc: 1.0000 - val_loss: 12.2739 - val_acc: 0.0773\n",
      "Epoch 64/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 12.2823 - val_acc: 0.0727\n",
      "Epoch 65/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 12.3341 - val_acc: 0.0773\n",
      "Epoch 66/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0069 - acc: 1.0000 - val_loss: 12.3552 - val_acc: 0.0727\n",
      "Epoch 67/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0063 - acc: 1.0000 - val_loss: 12.3796 - val_acc: 0.0727\n",
      "Epoch 68/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 12.4072 - val_acc: 0.0727\n",
      "Epoch 69/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 12.4246 - val_acc: 0.0727\n",
      "Epoch 70/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 12.4458 - val_acc: 0.0727\n",
      "Epoch 71/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 12.4690 - val_acc: 0.0727\n",
      "Epoch 72/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 12.4968 - val_acc: 0.0773\n",
      "Epoch 73/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 12.5210 - val_acc: 0.0773\n",
      "Epoch 74/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 12.5433 - val_acc: 0.0727\n",
      "Epoch 75/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 12.5762 - val_acc: 0.0727\n",
      "Epoch 76/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 12.6114 - val_acc: 0.0727\n",
      "Epoch 77/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 12.6189 - val_acc: 0.0727\n",
      "Epoch 78/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 12.6536 - val_acc: 0.0727\n",
      "Epoch 79/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 12.6717 - val_acc: 0.0727\n",
      "Epoch 80/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 12.7064 - val_acc: 0.0727\n",
      "Epoch 81/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 12.7115 - val_acc: 0.0727\n",
      "Epoch 82/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0016 - acc: 1.0000 - val_loss: 12.7388 - val_acc: 0.0727\n",
      "Epoch 83/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 12.7652 - val_acc: 0.0727\n",
      "Epoch 84/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 12.7795 - val_acc: 0.0727\n",
      "Epoch 85/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 12.7971 - val_acc: 0.0727\n",
      "Epoch 86/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 12.8150 - val_acc: 0.0727\n",
      "Epoch 87/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 0.0010 - acc: 1.0000 - val_loss: 12.8322 - val_acc: 0.0727\n",
      "Epoch 88/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 9.3519e-04 - acc: 1.0000 - val_loss: 12.8600 - val_acc: 0.0727\n",
      "Epoch 89/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 8.5956e-04 - acc: 1.0000 - val_loss: 12.8684 - val_acc: 0.0727\n",
      "Epoch 90/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 7.9092e-04 - acc: 1.0000 - val_loss: 12.8875 - val_acc: 0.0727\n",
      "Epoch 91/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 7.3298e-04 - acc: 1.0000 - val_loss: 12.9087 - val_acc: 0.0727\n",
      "Epoch 92/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 6.7377e-04 - acc: 1.0000 - val_loss: 12.9066 - val_acc: 0.0727\n",
      "Epoch 93/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 6.1860e-04 - acc: 1.0000 - val_loss: 12.9410 - val_acc: 0.0727\n",
      "Epoch 94/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 5.6989e-04 - acc: 1.0000 - val_loss: 12.9633 - val_acc: 0.0727\n",
      "Epoch 95/100\n",
      "3965/3965 [==============================] - 10s 3ms/step - loss: 5.2437e-04 - acc: 1.0000 - val_loss: 12.9778 - val_acc: 0.0727\n",
      "Epoch 96/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 4.8344e-04 - acc: 1.0000 - val_loss: 12.9808 - val_acc: 0.0727\n",
      "Epoch 97/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 4.4752e-04 - acc: 1.0000 - val_loss: 13.0107 - val_acc: 0.0727\n",
      "Epoch 98/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 4.1174e-04 - acc: 1.0000 - val_loss: 13.0280 - val_acc: 0.0727\n",
      "Epoch 99/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 3.7964e-04 - acc: 1.0000 - val_loss: 13.0448 - val_acc: 0.0727\n",
      "Epoch 100/100\n",
      "3965/3965 [==============================] - 10s 2ms/step - loss: 3.5183e-04 - acc: 1.0000 - val_loss: 13.0405 - val_acc: 0.0727\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(tr_input, tr_output, batch_size=32, epochs=100, validation_data=(va_input, va_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "of Dearth ( 1158-60 ) were at the time of this tale long past and the Hobbits had again become accustomed to plenty . The land was rich and kindly\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "seed_text = ngrams_samples[randint(0, len(ngrams_samples))]\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and though it had to been deserted when they entered it , it had before been well in order to provide the necessary background of 'history to its manner , and always plodded on , mostly by night , till i stood by balin 's tomb in moria .\n"
     ]
    }
   ],
   "source": [
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
